# -*- coding: utf-8 -*-
"""InceptUNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11CHn1oKSqy9qdQuEAOCUI2pNgGHidN2Y
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary

def main():
    net = IRNv2UNet(64)  # out is (B, 4608, 4, 4)
    summary(net, input_size=(120, 64, 96, 96))  # ~ 20GB forward/back passes.. how much do we have??

# ---------------------------------------------------------------------------------
class IRNv2_S1(nn.Module):
  def __init__(self, in_size, base=64):
    super(IRNv2_S1, self).__init__()
    # have this layer preserve size instead?

    self.stem_a = nn.Sequential(
        nn.Conv2d(in_size, base, kernel_size=5, stride=1, padding=2, bias=False), nn.ReLU(),
        nn.Conv2d(base, base, kernel_size=3, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(base, 2 * base, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        # nn.Conv2d(in_size, s1, kernel_size=3, stride=2, padding=0), nn.ReLU(),
        # nn.Conv2d(s1, s1, kernel_size=3, stride=1, padding=0), nn.ReLU(),
        # nn.Conv2d(s1, 2 * s1, kernel_size=3, stride=1, padding=1), nn.ReLU(),
        nn.BatchNorm2d(2 * base)
    )

  def forward(self, x):
    return self.stem_a(x)

# ---------------------------------------------------------------------------------
class IRNv2_S2(nn.Module):
  def __init__(self, in_size, base=64):
    super(IRNv2_S2, self).__init__()
    self.stem_b1 = nn.Sequential(
        nn.Conv2d(2 * base, 2 * base, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(2 * base)
    )
    self.stem_b2 = nn.Sequential(
        nn.Conv2d(2 * base, 3 * base, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(3 * base)
    )

  def forward(self, x):
    return torch.cat([self.stem_b1(x), self.stem_b2(x)], dim=1)

# ---------------------------------------------------------------------------------
class IRNv2_S3(nn.Module):
  def __init__(self, in_size, base=64):
    super(IRNv2_S3, self).__init__()
    self.stem_c1 = nn.Sequential(
        nn.Conv2d(in_size, 2 * base, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(2 * base, 3 * base, kernel_size=3, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(3 * base)
    )
    self.stem_c2 = nn.Sequential(
        nn.Conv2d(in_size, 2 * base, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(2 * base, 2 * base, kernel_size=(7, 1), stride=1, padding=(3, 0), bias=False), nn.ReLU(),
        nn.Conv2d(2 * base, 2 * base, kernel_size=(1, 7), stride=1, padding=(0, 3), bias=False), nn.ReLU(),
        nn.Conv2d(2 * base, 3 * base, kernel_size=3, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(3 * base)
    )

  def forward(self, x):
    return torch.cat([self.stem_c1(x), self.stem_c2(x)], dim=1)

# ---------------------------------------------------------------------------------
class IRNv2_S4(nn.Module):
  def __init__(self, in_size):
    super(IRNv2_S4, self).__init__()
    self.stem_d1 = nn.Sequential(  # this last Conv is pad 0 if using odd input shape
        nn.Conv2d(in_size, in_size, kernel_size=3, stride=2, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size)
    )
    self.stem_d2 = nn.MaxPool2d(2, stride=2, padding=0)

  def forward(self, x):
    return F.relu(torch.cat([self.stem_d1(x), self.stem_d2(x)], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2_A(nn.Module):
  def __init__(self, in_size, base=64):
    super(IRNv2_A, self).__init__()

    self.b1 = nn.Sequential(
        nn.Conv2d(in_size, base, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(base)
    )
    self.b2 = nn.Sequential(
        nn.Conv2d(in_size, base, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(base, base, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(base)
    )
    self.b3 = nn.Sequential(
        nn.Conv2d(in_size, base, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(base, 3 * base // 2, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.Conv2d(3 * base // 2, 2 * base, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(2 * base)
    )
    self.comb = nn.Conv2d(4 * base, in_size, kernel_size=1, stride=1, padding=0, bias=False)
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x):
    x_conv = torch.cat([self.b1(x), self.b2(x), self.b3(x)], dim=1)  # filter cat right?
    x_conv = 0.1 * self.comb(x_conv) # linear activation scaling for stability

    return self.final(x + x_conv)  # residual connection

# ---------------------------------------------------------------------------------
class IRNv2_redA(nn.Module):
  def __init__(self, in_size):
    super(IRNv2_redA, self).__init__()
    self.b1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
    self.b2 = nn.Sequential(
        nn.Conv2d(in_size, in_size, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size)
    )
    self.b3 = nn.Sequential(
        nn.Conv2d(in_size, 3 * in_size // 2, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(3 * in_size // 2, 3 * in_size // 2, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.Conv2d(3 * in_size // 2, in_size, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size)
    )
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x):
    return self.final(torch.cat([self.b1(x), self.b2(x), self.b3(x)], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2_B(nn.Module):
  def __init__(self, in_size):
    super(IRNv2_B, self).__init__()
    self.b1 = nn.Sequential(
        nn.Conv2d(in_size, in_size // 8, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 8)
    )
    self.b2 = nn.Sequential(
        nn.Conv2d(in_size, in_size // 10, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(in_size // 10, in_size // 9, kernel_size=(7, 1), stride=1, padding=(3, 0), bias=False), nn.ReLU(),
        nn.Conv2d(in_size // 9, in_size // 8, kernel_size=(1, 7), stride=1, padding=(0, 3), bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 8)
    )
    self.comb = nn.Conv2d(in_size // 4, in_size, kernel_size=1, stride=1, padding=0, bias=False)
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x):
    x_conv = torch.cat([self.b1(x), self.b2(x)], dim=1)
    x_conv = 0.1 * self.comb(x_conv)  # linear activation scaling for stability
    return self.final(x + x_conv)

# ---------------------------------------------------------------------------------
class IRNv2_redB(nn.Module):
  def __init__(self, in_size):
    super(IRNv2_redB, self).__init__()
    self.b1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=0)
    self.b2 = nn.Sequential(
        nn.Conv2d(in_size, 2 * in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(2 * in_size // 12, 5 * in_size // 12, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(5 * in_size // 12)
    )
    self.b3 = nn.Sequential(
        nn.Conv2d(in_size, 2 * in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(2 * in_size // 12, 3 * in_size // 12, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(3 * in_size // 12)
    )
    self.b4 = nn.Sequential(
        nn.Conv2d(in_size, 2 * in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(2 * in_size // 12, 3 * in_size // 12, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.Conv2d(3 * in_size // 12, 4 * in_size // 12, kernel_size=3, stride=2, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(4 * in_size // 12)
    )
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x):
    return self.final(torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2_C(nn.Module):
  def __init__(self, in_size):
    super(IRNv2_C, self).__init__()
    self.b1 = nn.Sequential(
        nn.Conv2d(in_size, in_size // 10, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 10)
    )
    self.b2 = nn.Sequential(
        nn.Conv2d(in_size, in_size // 10, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.Conv2d(in_size // 10, in_size // 9, kernel_size=(1, 3), stride=1, padding=(0, 1), bias=False), nn.ReLU(),
        nn.Conv2d(in_size // 9, in_size // 8, kernel_size=(3, 1), stride=1, padding=(1, 0), bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 8)
    )
    cat_size = in_size // 8 + in_size // 10
    self.comb = nn.Conv2d(cat_size, in_size, kernel_size=1, stride=1, padding=0, bias=False)
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x):
    x_conv = torch.cat([self.b1(x), self.b2(x)], dim=1)
    x_conv = 0.1 * self.comb(x_conv)  # linear activation scaling for stability
    return self.final(x + x_conv)

# ---------------------------------------------------------------------------------
class IRNv2_revLB(nn.Module):
  def __init__(self, in_size, out_size):
    super(IRNv2_revLB, self).__init__()
    self.b1 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 12)
    )
    self.b2 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size // 12, in_size // 6, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 6)
    )
    self.b3 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size //12, in_size // 8, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size // 8, in_size // 4, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 4)
    )
    # self.ct = nn.Sequential(
    #     nn.ConvTranspose2d(in_size, out_size, kernel_size=3, stride=1, padding=1), nn.ReLU(),
    #     nn.BatchNorm2d(out_size)
    # )
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x, cat_in):
    comb = torch.cat([x, cat_in], dim=1)
    return self.final(torch.cat([self.b1(comb), self.b2(comb), self.b3(comb)], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2_revLA(nn.Module):
  def __init__(self, in_size, out_size):
    super(IRNv2_revLA, self).__init__()
    self.b1 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 6, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 6)
    )
    self.b2 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 12, kernel_size=1, stride=1, padding=0, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size // 12, in_size // 8, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size // 8, in_size // 3, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // 3)
    )
    # self.ct = nn.Sequential(
    #     nn.ConvTranspose2d(in_size, out_size, kernel_size=3, stride=1, padding=1), nn.ReLU(),
    #     nn.BatchNorm2d(out_size)
    # )
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x, cat_in):
    comb = torch.cat([x, cat_in], dim=1)
    return self.final(torch.cat([self.b1(comb), self.b2(comb)], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2_revRL(nn.Module):
  def __init__(self, in_size, out_size):
    super(IRNv2_revRL, self).__init__()
    # TODO: worth breaking this up like A/B reduction ones? 
    ratio = in_size // out_size
    hid = 2 * ratio
    self.b1 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 24, kernel_size=3, stride=2, padding=0, output_padding=1, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size // 24, in_size // hid, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // hid)
    )
    self.b2 = nn.Sequential(
        nn.ConvTranspose2d(in_size, in_size // 24, kernel_size=3, stride=1, padding=1, bias=False), nn.ReLU(),
        nn.ConvTranspose2d(in_size // 24, in_size // hid, kernel_size=3, stride=2, padding=0, output_padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(in_size // hid)
    ) 
    # self.ct = nn.Sequential(
    #     nn.ConvTranspose2d(in_size, out_size, kernel_size=3, stride=2, padding=0, output_padding=1), nn.ReLU(),
    #     nn.BatchNorm2d(out_size)
    # )
    self.final = nn.Sequential(nn.ReLU(), nn.Dropout2d(p=0.2))

  def forward(self, x):
    return self.final(torch.cat([self.b1(x), self.b2(x)], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2_revS(nn.Module):
  def __init__(self, in_size, out_size, stride=1, padding=1):
    super(IRNv2_revS, self).__init__()
    self.ct = nn.Sequential(
        nn.ConvTranspose2d(in_size, out_size, kernel_size=3, stride=stride, padding=padding, output_padding=1, bias=False), nn.ReLU(),
        nn.BatchNorm2d(out_size)
    )

  def forward(self, x):
    return self.ct(x)

# ---------------------------------------------------------------------------------
class IRNv2_revScat(nn.Module):
  def __init__(self, in_size, out_size, stride=1):
    super(IRNv2_revScat, self).__init__()
    self.ct = nn.Sequential(
        nn.ConvTranspose2d(in_size, out_size, kernel_size=3, stride=stride, padding=0, bias=False), nn.ReLU(),
        nn.BatchNorm2d(out_size)
    )

  def forward(self, x, cat_in):
    return self.ct(torch.cat([x, cat_in], dim=1))

# ---------------------------------------------------------------------------------
class IRNv2UNet(nn.Module):
  def __init__(self, in_size, Na=3, Nb=5, Nc=3):
    """
    Na, Nb, Nc: number of times to run through layers A, B, C
    Inception-ResNet-v1/2 use 5, 10, 5
    """
    super(IRNv2UNet, self).__init__()
    base = in_size
    self.Na, self.Nb, self.Nc = Na, Nb, Nc
    # down layers
    self.s1 = IRNv2_S1(in_size, base)  # (B, 256, 45, 45) (256, 94, 94) OR (128, 94, 94)
    self.s2 = IRNv2_S2(128, base)  # (B, 640, 22, 22) (640, 46, 46) OR (320, 46, 46)
    self.s3 = IRNv2_S3(320, base)  # (B, 768, 20, 20) (768, 44, 44) OR (384, 44, 44)
    self.s4 = IRNv2_S4(384)  # (B, 1536, 10, 10) (1536, 22, 22) OR (768, 22, 22)
    self.a_n = [IRNv2_A(768, base) for _ in range(Na)]  # (B, 1536, 10, 10) (1536, 22, 22) OR (768, 22, 22)
    self.ra = IRNv2_redA(768)  # (B, 4608, 4, 4) (4608, 10, 10) OR (2304, 10, 10)
    self.b_n = [IRNv2_B(2304) for _ in range(Nb)] # (4608, 10, 10) OR (2304, 10, 10)
    self.rb = IRNv2_redB(2304) # (9216, 4, 4)  consider removing this? OR (4608, 4, 4)
    self.c_n = [IRNv2_C(4608) for _ in range(Nc)] # (4608, 4, 4)

    # up layers -- possible to 'reverse' the inception blocks instead?
    # could replace the ConvTranspose2d with Upsample where small change in input size?
    self.rev_rb = IRNv2_revRL(4608, 2304)  # (B, 2304, 10, 10)
    self.rev_b = IRNv2_revLB(4608, 2304)  # (B, 2304, 10, 10)
    self.rev_ra = IRNv2_revRL(2304, 768)  # (B, 768, 22, 22)
    self.rev_a = IRNv2_revLA(1536, 768)  # (B, 768, 22, 22)
    self.rev_s4 = IRNv2_revS(768, 384, stride=2, padding=1)  # (B, 384, 44, 44)
    self.rev_s3 = IRNv2_revScat(768, 320, stride=1)  # (B, 320, 46, 46)
    self.rev_s2 = IRNv2_revS(320, 128, stride=2, padding=0)  # (B, 128, 94, 94)
    self.rev_s1 = IRNv2_revScat(256, in_size, stride=1)  # (B, 64, 96, 96)

    self.final = nn.Conv2d(in_size, 1, kernel_size=3, stride=1, padding=1) # (B, 1, 96, 96)

    self.net_down = nn.ModuleList([self.s1, self.s2, self.s3, self.s4] + self.a_n + [self.ra] + self.b_n + [self.rb] + self.c_n)

    # do the same down here.?
    self.net_up = nn.ModuleList([
        self.rev_rb, self.rev_b, self.rev_ra, self.rev_a,
        self.rev_s4, self.rev_s3, self.rev_s2, self.rev_s1
    ])

  def forward(self, x):
    outs = []
    for i, step in enumerate(self.net_down):
      x = step(x)
      if i in [0, 2, 3 + self.Na, 3 + self.Na + 1 + self.Nb]:
        outs.append(x)

    ctr = 0
    for j, step in enumerate(self.net_up):
      if (j + 1) % 2 == 0:
        ctr += 1
        x = step(x, outs[-ctr])
      else:
        x = step(x)

    return self.final(x)

# ---------------------------------------------------------------------------------
if __name__ == '__main__':
    main()

